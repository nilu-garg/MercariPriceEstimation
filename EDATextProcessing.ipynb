{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### General Libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import logging\n",
    "logging.getLogger(\"lda\").setLevel(logging.WARNING)\n",
    "\n",
    "#### Libraries for plotting graphs\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import plotly.offline as py\n",
    "py.init_notebook_mode(connected=True)\n",
    "import plotly.graph_objs as go\n",
    "import plotly.tools as tls\n",
    "%matplotlib inline\n",
    "\n",
    "import bokeh.plotting as bp\n",
    "from bokeh.models import HoverTool, BoxSelectTool\n",
    "from bokeh.models import ColumnDataSource\n",
    "from bokeh.plotting import figure, show, output_notebook\n",
    "#from bokeh.transform import factor_cmap\n",
    "\n",
    "### Libraries for handling text\n",
    "\n",
    "import string, re\n",
    "import nltk\n",
    "from nltk.stem.porter import *\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction import stop_words\n",
    "from collections import Counter\n",
    "from wordcloud import WordCloud\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('train.tsv',sep = '\\t')\n",
    "test = pd.read_csv('test.tsv',sep = '\\t')\n",
    "\n",
    "print(\"Train has %d rows and %d columns.\" % (train.shape[0], train.shape[1]))\n",
    "print(\"Test has %d rows and %d columns.\" %(test.shape[0], test.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relationship of price with Item Description\n",
    "\n",
    "Item description has 4 null values. 82489 rows have \"No description yet\". \n",
    "The item description has a meean length of 13 words with one item being decribed in upto 130 words.\n",
    "There isn't a lot of difference between price based on descriptions\n",
    "\n",
    "Removing stop words from a sentence takes emotionn out of it. Since it's the description of an item, just the words matter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Cleaning and Standardizing text\n",
    "\n",
    "def wordClean(text):\n",
    "    try:\n",
    "        text = text.lower()\n",
    "        regex = re.compile('[' +re.escape(string.punctuation) + '0-9\\\\r\\\\t\\\\n]')\n",
    "        return regex.sub(\" \",text)        \n",
    "    except:\n",
    "        return \"\"\n",
    "    \n",
    "def wordCount(text):\n",
    "    try:\n",
    "        #words = nltk.word_tokenize()\n",
    "        words = [w for w in text.split(\" \") if not w in stop_words.ENGLISH_STOP_WORDS and len(w)>3]\n",
    "        return len(words)        \n",
    "    except:J\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['clean_text'] = train['item_description'].apply(lambda x: wordClean(x))\n",
    "train['description_length'] = train['clean_text'].apply(lambda x: wordCount(x))\n",
    "train['description_length'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = train.groupby('description_length')['price'].mean().reset_index()\n",
    "\n",
    "data = [go.Scatter(x = df['description_length'],y = np.log(df['price']+1), mode = 'lines+markers',name = 'lines+markers')]\n",
    "layout = dict(title= 'Average Log(Price) by Description Length',\n",
    "              yaxis = dict(title='Average Log(Price)'),\n",
    "              xaxis = dict(title='Description Length'))\n",
    "fig=dict(data=data, layout=layout)\n",
    "py.iplot(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.loc[train['clean_text'] == \"no description yet\",'clean_text'] = \"\"\n",
    "train['clean_stop_words'] = train['clean_text'].apply(lambda x: [w for w in x.split(\" \") if not w in stop_words.ENGLISH_STOP_WORDS and len(w)>3])\n",
    "train['clean_stop_words'] = train['clean_stop_words'].apply(lambda x: \" \".join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def category_split(x):\n",
    "    try: return(x.split(\"/\"))\n",
    "    except: return((\"NA\",\"NA\",\"NA\"))\n",
    "train['Cat_1'], train['Cat_2'],train['Cat_3'] = zip(*train['category_name'].apply(lambda x: category_split(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "general_categories = train['Cat_1'].unique()\n",
    "# create a dictionary of words for each category\n",
    "cat_desc = dict()\n",
    "for cat in general_categories: \n",
    "    text = \" \".join(train.loc[train['Cat_1']==cat, 'clean_stop_words'].values)\n",
    "    cat_desc[cat] = nltk.word_tokenize(text)\n",
    "\n",
    "# flat list of all words combined\n",
    "flat_list = [item for sublist in list(cat_desc.values()) for item in sublist]\n",
    "allWordsCount = Counter(flat_list)\n",
    "all_top10 = allWordsCount.most_common(20)\n",
    "x = [w[0] for w in all_top10]\n",
    "y = [w[1] for w in all_top10]\n",
    "\n",
    "data = [go.Bar(x=x, y=y)]\n",
    "layout = dict(title= 'Word Frequency',yaxis = dict(title='Count'),xaxis = dict(title='Word'))\n",
    "fig=dict(data=data, layout=layout)\n",
    "py.iplot(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = set(stopwords.words('english'))\n",
    "def tokenize(text):\n",
    "    try: \n",
    "        text = wordClean(text)         \n",
    "        tokens = list(filter(lambda t: t not in stop, nltk.word_tokenize(text)))\n",
    "        filtered_tokens = [w for w in tokens if re.search('[a-zA-Z]', w)]\n",
    "        filtered_tokens = [w for w in filtered_tokens if len(w)>=3]        \n",
    "        return filtered_tokens\n",
    "            \n",
    "    except TypeError as e: print(text,e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['tokens'] = train['item_description'].map(tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for description, tokens in zip(train['item_description'].head(),train['tokens'].head()):\n",
    "    print('description:', description)\n",
    "    print('tokens:', tokens)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_desc = dict()\n",
    "for cat in general_categories: \n",
    "    text = \" \".join(train.loc[train['Cat_1']==cat, 'clean_text'].values)\n",
    "    cat_desc[cat] = tokenize(text)\n",
    "\n",
    "# find the most common words for the top 4 categories\n",
    "women100 = Counter(cat_desc['Women']).most_common(100)\n",
    "beauty100 = Counter(cat_desc['Beauty']).most_common(100)\n",
    "kids100 = Counter(cat_desc['Kids']).most_common(100)\n",
    "electronics100 = Counter(cat_desc['Electronics']).most_common(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_wordcloud(tup):\n",
    "    wordcloud = WordCloud(background_color='white',max_words=50, max_font_size=40,\n",
    "                          random_state=42).generate(str(tup))\n",
    "    return wordcloud\n",
    "\n",
    "fig,axes = plt.subplots(2, 2, figsize=(30, 15))\n",
    "\n",
    "ax = axes[0, 0]\n",
    "ax.imshow(generate_wordcloud(women100), interpolation=\"bilinear\")\n",
    "ax.axis('off')\n",
    "ax.set_title(\"Women Top 100\", fontsize=30)\n",
    "\n",
    "ax = axes[0, 1]\n",
    "ax.imshow(generate_wordcloud(beauty100))\n",
    "ax.axis('off')\n",
    "ax.set_title(\"Beauty Top 100\", fontsize=30)\n",
    "\n",
    "ax = axes[1, 0]\n",
    "ax.imshow(generate_wordcloud(kids100))\n",
    "ax.axis('off')\n",
    "ax.set_title(\"Kids Top 100\", fontsize=30)\n",
    "\n",
    "ax = axes[1, 1]\n",
    "ax.imshow(generate_wordcloud(electronics100))\n",
    "ax.axis('off')\n",
    "ax.set_title(\"Electronic Top 100\", fontsize=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF (Term Frequency-Inverse Document Frequency)\n",
    "\n",
    "TfidfVectorizer takes in input df, tokenizer, analyser, stop_words, max_features, ngram_range\n",
    "\n",
    "For the given tokenizer\n",
    "1. ngram_range is selecting unigrams and bigrams\n",
    "2. min_df is going to ignore tokens that have coount less than 10. (literature cut off)\n",
    "3. max_features is the number of tokens that is going to be used to create a vocabulary based on token frequency\n",
    "4. tokenizer is the preporicessing that overrides string tokenization step\n",
    "5. decode_error will ignore empty descriptions\n",
    "\n",
    "the output of the vectorizer is a matrix with each column as a feature(token) and its value for each row(item description)\n",
    "\n",
    "Once the vectorizatino is done, we can map the tokens to their tfidf values. Terms with low tfidf values will be generic terms that cannot be used to distinguish items, whereas terms with high tfidf should be able to distinguish items and categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(min_df=10,max_features=180000,tokenizer=tokenize,ngram_range=(1, 2),decode_error = 'ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.loc[train['item_description'].isnull(),'item_description'] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_desc = np.append(train['item_description'].values, test['item_description'].values)\n",
    "vz = vectorizer.fit_transform(list(all_desc))\n",
    "\n",
    "## mapping the tokens to idf values\n",
    "tfidf = dict(zip(vectorizer.get_feature_names(), vectorizer.idf_))\n",
    "tfidf = pd.DataFrame(columns=['tfidf']).from_dict(dict(tfidf), orient='index')\n",
    "tfidf.columns = ['tfidf']\n",
    "\n",
    "tfidf.sort_values(by=['tfidf'], ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## mapping the tokens to idf values\n",
    "tfidf = dict(zip(vectorizer.get_feature_names(), vectorizer.idf_))\n",
    "tfidf = pd.DataFrame(columns=['tfidf']).from_dict(dict(tfidf), orient='index')\n",
    "tfidf.columns = ['tfidf']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf.sort_values(by=['tfidf'], ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dimensinality Recution\n",
    "\n",
    "SVD (Singular Value Decomposition) is PCA but numerically stable. Using sklearn SVD - reducing the features from 4232 to 30 helps in applying t-SNE to the already reduced data. \n",
    "\n",
    "t-SNE cost function is not convex - that means different initializatinos leads to different results. It is compuationally expensive, so converting high dimensional matrics using SVD (for sparse data) or PCA (for dense data) is recomended before using t-SNE.\n",
    "\n",
    "for t-SNE\n",
    "1. n_components is the dimensionality of the embedded space\n",
    "2. perplexity\n",
    "3. learning_rate\n",
    "4. n_iter - for optimization purposes\n",
    "5. init - the initial matrix can be defined at the start\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn = train.copy()\n",
    "tst = test.copy()\n",
    "trn['is_train'] = 1\n",
    "tst['is_train'] = 0\n",
    "\n",
    "sample_sz = 15000\n",
    "\n",
    "combined_sample = pd.concat([trn, tst]).sample(n=sample_sz)\n",
    "vz_sample = vectorizer.fit_transform(list(combined_sample['item_description']))\n",
    "\n",
    "vz_sample.shape\n",
    "\n",
    "svd = TruncatedSVD(n_components=30, random_state=42)\n",
    "svd_tfidf = svd.fit_transform(vz_sample)\n",
    "\n",
    "svd_tfidf.shape\n",
    "tsne_model = TSNE(n_components=2, verbose=1, random_state=42, n_iter=500)\n",
    "\n",
    "tsne_tfidf = tsne_model.fit_transform(svd_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#combined_sample.reset_index(inplace=True, drop=True)\n",
    "tfidf_df = pd.DataFrame(tsne_tfidf, columns=['x', 'y'])\n",
    "tfidf_df['description'] = combined_sample['item_description']\n",
    "tfidf_df['tokens'] = combined_sample['tokens']\n",
    "tfidf_df['category'] = combined_sample['Cat_1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#output_notebook()\n",
    "#plot_tfidf = bp.figure(plot_width=700, plot_height=600,\n",
    "#                       title=\"tf-idf clustering of the item description\",\n",
    "#    tools=\"pan,wheel_zoom,box_zoom,reset,hover,previewsave\",\n",
    "#    x_axis_type=None, y_axis_type=None, min_border=1)\n",
    "\n",
    "#plot_tfidf.scatter(x='x', y='y', source=tfidf_df, alpha=0.7)\n",
    "#hover = plot_tfidf.select(dict(type=HoverTool))\n",
    "#hover.tooltips={\"description\": \"@description\", \"tokens\": \"@tokens\", \"category\":\"@category\"}\n",
    "#show(plot_tfidf)\n",
    "\n",
    "tf1 = tfidf_df[tfidf_df['category'].notnull()]#.plot(x=x, y=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sns.scatterplot(x = \"x\", y = \"y\",data = tf1, hue = 'category')\n",
    "import random\n",
    "color = [\"#\" + \"\".join([random.choice(\"0123456789ABCDEF\") for j in range(6)])\n",
    "    for i in range(len(general_categories))\n",
    "]\n",
    "\n",
    "cols = tf1[\"category\"].map({clust: color for clust, color in zip(general_categories, color)})\n",
    "\n",
    "data = [go.Scatter(x = tf1['x'],y = tf1['y'], mode = 'markers',marker=dict(size=15, color=cols),name = 'markers')]\n",
    "layout = dict(title= 'T-SNE Plot',\n",
    "              yaxis = dict(title='Y-axis'),\n",
    "              xaxis = dict(title='X-axis'))\n",
    "fig=dict(data=data, layout=layout)\n",
    "py.iplot(fig)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
